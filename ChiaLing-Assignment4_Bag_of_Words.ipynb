{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 2000\n",
      "dict_keys(['target', 'data', 'filenames', 'DESCR', 'target_names'])\n",
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn import metrics\n",
    "\n",
    "cur_path = os.getcwd()\n",
    "movie_reviews_data_folder = r\"%s\\text_analytics\\data\\movie_reviews\\txt_sentoken\"  %cur_path\n",
    "\n",
    "dataset = load_files(movie_reviews_data_folder, shuffle=False)\n",
    "print(\"n_samples: %d\" % len(dataset.data))\n",
    "print(dataset.keys())\n",
    "print(dataset.target_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "        dataset.data, dataset.target, test_size=0.25, random_state=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing text with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9953)\t3\n",
      "  (0, 4522)\t3\n",
      "  (0, 30979)\t1\n",
      "  (0, 14759)\t6\n",
      "  (0, 31570)\t1\n",
      "  (0, 23293)\t2\n",
      "  (0, 34807)\t5\n",
      "  (0, 21298)\t7\n",
      "  (0, 18583)\t4\n",
      "  (0, 2567)\t5\n",
      "  (0, 1570)\t15\n",
      "  (0, 18293)\t1\n",
      "  (0, 24193)\t3\n",
      "  (0, 32636)\t1\n",
      "  (0, 16642)\t6\n",
      "  (0, 16599)\t11\n",
      "  (0, 34927)\t1\n",
      "  (0, 5806)\t1\n",
      "  (0, 25708)\t1\n",
      "  (0, 15109)\t1\n",
      "  (0, 33159)\t1\n",
      "  (0, 34919)\t3\n",
      "  (0, 31479)\t16\n",
      "  (0, 11772)\t4\n",
      "  (0, 9779)\t1\n",
      "  :\t:\n",
      "  (1499, 6763)\t1\n",
      "  (1499, 4716)\t1\n",
      "  (1499, 7357)\t1\n",
      "  (1499, 34911)\t1\n",
      "  (1499, 30310)\t1\n",
      "  (1499, 32853)\t1\n",
      "  (1499, 9478)\t1\n",
      "  (1499, 25678)\t1\n",
      "  (1499, 25758)\t1\n",
      "  (1499, 26221)\t1\n",
      "  (1499, 23668)\t1\n",
      "  (1499, 11007)\t1\n",
      "  (1499, 13393)\t1\n",
      "  (1499, 28796)\t1\n",
      "  (1499, 27738)\t1\n",
      "  (1499, 18188)\t1\n",
      "  (1499, 5924)\t1\n",
      "  (1499, 6060)\t1\n",
      "  (1499, 25217)\t1\n",
      "  (1499, 17870)\t1\n",
      "  (1499, 10257)\t1\n",
      "  (1499, 14170)\t1\n",
      "  (1499, 32808)\t1\n",
      "  (1499, 12571)\t1\n",
      "  (1499, 4430)\t1\n",
      "(1500, 35357)\n",
      "20612\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(docs_train)\n",
    "print (X_train_counts)\n",
    "print (X_train_counts.shape)\n",
    "print(count_vect.vocabulary_.get(u'movie'))  # Find out the movie occurences number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From occurrences to frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 35505)\n",
      "  (0, 16926)\t0.095214108814\n",
      "  (0, 18256)\t0.11108312695\n",
      "  (0, 4600)\t0.0158690181357\n",
      "  (0, 21992)\t0.0158690181357\n",
      "  (0, 31632)\t0.650629743563\n",
      "  (0, 1480)\t0.0317380362713\n",
      "  (0, 832)\t0.126952145085\n",
      "  (0, 20719)\t0.095214108814\n",
      "  (0, 27427)\t0.0158690181357\n",
      "  (0, 34651)\t0.0634760725427\n",
      "  (0, 14407)\t0.0317380362713\n",
      "  (0, 30166)\t0.0158690181357\n",
      "  (0, 28394)\t0.0317380362713\n",
      "  (0, 15789)\t0.142821163221\n",
      "  (0, 197)\t0.0158690181357\n",
      "  (0, 18208)\t0.0158690181357\n",
      "  (0, 34485)\t0.0158690181357\n",
      "  (0, 34938)\t0.0634760725427\n",
      "  (0, 14813)\t0.158690181357\n",
      "  (0, 34768)\t0.0158690181357\n",
      "  (0, 18669)\t0.0158690181357\n",
      "  (0, 21048)\t0.0158690181357\n",
      "  (0, 20718)\t0.0158690181357\n",
      "  (0, 1592)\t0.222166253899\n",
      "  (0, 32998)\t0.0158690181357\n",
      "  :\t:\n",
      "  (1499, 20336)\t0.00674660014852\n",
      "  (1499, 32885)\t0.00674660014852\n",
      "  (1499, 12054)\t0.00674660014852\n",
      "  (1499, 31028)\t0.00674660014852\n",
      "  (1499, 20744)\t0.00674660014852\n",
      "  (1499, 19803)\t0.00674660014852\n",
      "  (1499, 34379)\t0.00674660014852\n",
      "  (1499, 12052)\t0.00674660014852\n",
      "  (1499, 20366)\t0.00674660014852\n",
      "  (1499, 16642)\t0.013493200297\n",
      "  (1499, 22416)\t0.00674660014852\n",
      "  (1499, 17549)\t0.00674660014852\n",
      "  (1499, 25949)\t0.00674660014852\n",
      "  (1499, 23199)\t0.00674660014852\n",
      "  (1499, 16404)\t0.00674660014852\n",
      "  (1499, 25806)\t0.00674660014852\n",
      "  (1499, 10674)\t0.00674660014852\n",
      "  (1499, 30678)\t0.00674660014852\n",
      "  (1499, 4456)\t0.00674660014852\n",
      "  (1499, 31535)\t0.00674660014852\n",
      "  (1499, 5863)\t0.00674660014852\n",
      "  (1499, 21632)\t0.00674660014852\n",
      "  (1499, 33410)\t0.00674660014852\n",
      "  (1499, 21245)\t0.00674660014852\n",
      "  (1499, 18431)\t0.00674660014852\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "print(X_train_tf.shape)\n",
    "print(X_train_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=2, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "knn.fit(X_train_tf, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge Above Jobs into Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "                     ('knn', KNeighborsClassifier(n_neighbors=5)),\n",
    "           ])\n",
    "\n",
    "text_clf = text_clf.fit(docs_train,  y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57199999999999995"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == y_test)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.63      0.30      0.40       244\n",
      "        pos       0.55      0.84      0.67       256\n",
      "\n",
      "avg / total       0.59      0.57      0.54       500\n",
      "\n",
      "[[ 72 172]\n",
      " [ 42 214]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7caa898>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPYAAAD7CAYAAABZjGkWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABXBJREFUeJzt2z+LnXUexuH7uwlW+gJEAik2glZW1lNmK0tJt/YWdraz\n78FWLXVbm5Aui2BlsSCsQlIMRBErX4DCb4tk2Qg5OYecM/Nk770uODDnD8/cMPPheWbmzKy1AnT5\n09YDgNMTNhQSNhQSNhQSNhQSNhQS9nPMzO2Z+WFmHszMx1vv4TAz89nM/DIz3229ZSvC3mFmriX5\nJMntJG8nuTMzb227igN9nsdft/9bwt7t3SQP11oXa63fknyZ5L2NN3GAtdbXSX7deseWhL3bG0ke\nPXX/xyePwUtP2Lt5ry3/s4S9209Jbjx1/0Yen7XhpSfs3b5Ncmtmbs7MK0neT/LVxpvgIMLeYa31\ne5IPk9xL8q8kf19rfb/tKg4xM18k+SbJmzPzaGY+2HrTVRv/tgl9nLGhkLChkLChkLChkLCh0PVj\nDzAzfq0OG1lrzbMePzrsJMk7xW3/fJ68fr71ikuz/vrM74sa53eT879sveJyzEe7n3MpDoWEDYWE\nvc+rZ1sv4Ahnf956wTaEvc9rZ1sv4Ahnt7ZesA1hQyFhQyFhQyFhQyFhQyFhQyFhQyFhQyFhQyFh\nQyFhQyFhQyFhQyFhQyFhQyFhQyFhQyFhQyFhQyFhQyFhQyFhQyFhQyFhQyFhQyFhQyFhQyFhQyFh\nQyFhQyFhQyFhQyFhQyFhQyFhQyFhQyFhQyFhQyFhQyFhQyFhQyFhQyFhQyFhQyFhQ6G9Yc/M7Zn5\nYWYezMzHVzEKOM5zw56Za0k+SXI7ydtJ7szMW1cxDHhx+87Y7yZ5uNa6WGv9luTLJO9d/izgGPvC\nfiPJo6fu//jkMeAldn3P8+ugo/x8/t+PXz1LXjt7wTnALvcfJPcfHvbafWH/lOTGU/dv5PFZ+49e\nPz/sswEv7OzW49t//O3e7tfuuxT/Nsmtmbk5M68keT/JV8dPBC7Tc8/Ya63fZ+bDJPeSXEvy6Vrr\n+ytZBrywfZfiWWvdTXL3CrYAJ+KdZ1BI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI\n2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI\n2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI\n2FDo+kmO8s/zkxyGq3f+0dYLuAzO2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI\n2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI\n2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI2FBI\n2FBob9gz89nM/DIz313FIOB4h5yxP09y+7KHAKezN+y11tdJfr2CLcCJ+BkbCl0/zWHuP/XxzSc3\n4JQuntwOcaKwz05zGGCnm/njKfMfz3mtS3EodMifu75I8k2SN2fm0cx8cPmzgGPsvRRfa925iiHA\n6bgUh0LChkLChkLChkLChkLChkLChkLChkLChkLChkLChkLChkLChkLChkLChkLChkLChkLChkLC\nhkLChkLChkLChkLChkLChkLChkLChkLChkLChkLChkLChkLChkLChkLChkLChkLChkLChkLChkLC\nhkLChkLChkLChkLChkLChkLChkLChkLChkLChkLC3uti6wEc4WLrARsR9l4XWw/gCBdbD9iIsKGQ\nsKHQrLWOO8DMcQcAXthaa571+NFhAy8fl+JQSNhQSNhQSNhQSNhQ6N8jI4uPOkBNTgAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x86df860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, predicted,\n",
    "                                        target_names=dataset.target_names))\n",
    "# Print and plot the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, predicted)\n",
    "print(cm)\n",
    "plt.matshow(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5, 1.0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "filelist = [[1, 1], [2,1] ]\n",
    "filelist_matrix = np.matrix(filelist)\n",
    "result = []\n",
    "result = np.mean(filelist , axis = 0).tolist()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
