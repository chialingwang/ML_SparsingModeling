{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Data Partition as Training and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\patch_database\n",
      "..\\patch_database\\sample01_002_3x3\n",
      "..\\patch_database\\sample26_002_3x3\n",
      "[ 1  1  1  1  2  2  2  2  3  3  3  3  4  4  4  4  5  5  5  5  6  6  6  6  7\n",
      "  7  7  7  8  8  8  8  9  9  9  9 10 10 10 10 11 11 11 11 12 12 12 12 13 13\n",
      " 13 13 14 14 14 14 15 15 15 15 16 16 16 16 17 17 17 17 18 18 18 18 19 19 19\n",
      " 19 20 20 20 20 21 21 21 21 22 22 22 22 23 23 23 23 24 24 24 24 25 25 25 25]\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import image_norm_test as myData;\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "seed = 3071986   # set random seed\n",
    "rng = np.random.RandomState(seed)\n",
    "import re\n",
    "import fetch_file\n",
    "\n",
    "# fetch all the file in the directory path  defined#\n",
    "pwd = os.pardir;\n",
    "accessPath = r\"%s\\patch_database\" %pwd;\n",
    "filelist = []\n",
    "print(accessPath)\n",
    "\n",
    "\n",
    "# in this way we can extract only certain number image of each samples, ex. 2 means only 2 images of one texture\n",
    "filelist , lable = fetch_file.fetchFile(accessPath , 4)\n",
    "print(filelist[0])\n",
    "print(filelist[100])\n",
    "filelist = np.array(filelist).transpose()[0:100];\n",
    "lable = np.array(lable).transpose()[0:100];\n",
    "print(lable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9  2  7  5 24 19  2 10  2 21 20 25 23 23 24 16  3 25  1 17 20 16 12 23  9\n",
      " 19  6 15 12 14 25 10 15  8 17 22 17 11 25 10 21  5  1  4  6 18 20  4 13 14\n",
      " 18  9  5  9 20 21 24 18  8  7 12 22  7 16  8 18  1 13  3 16 12 22 23  3  8]\n",
      "[19 13 11  2 10 21 11 24  3  1 14  7 15  4 13  6 17 11 22 15 14  5  6  4 19]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "rng = np.random.RandomState(seed)\n",
    "permutation = rng.permutation(len(filelist))\n",
    "\n",
    "#print(permutation)\n",
    "#print(filelist[permutation])\n",
    "X, y = filelist[permutation], lable[permutation]\n",
    "\n",
    "\n",
    "# split our data into half of train data and half of text data randomly.\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, train_size=0.75, random_state=seed)\n",
    "\n",
    "\n",
    "print(train_y)\n",
    "print(test_y)\n",
    "#print(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start to get the features learned from feed in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrive centroids ( features ) from file stored in disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.359201245835, -0.350276323095, -0.284874560613, -0.387853317701, -0.356222505484, -0.265256232066, -0.382704706903, -0.336148081943, -0.244915257572], [0.358806866422, 0.290237064273, 0.279483142322, 0.341993511392, 0.295504326746, 0.315070116924, 0.35871147642, 0.356818418472, 0.386476904451], [-0.148784868061, 0.164467159988, 0.442070713355, -0.131500140521, 0.248641382045, 0.541956821763, -0.0695002573605, 0.294172120952, 0.539617973866], [-0.37811067742, -0.182680751809, 0.262972398401, -0.36602944445, 0.000716159993457, 0.476164862946, -0.207526841013, 0.192900624991, 0.55992543393], [0.211746974514, 0.364218791337, 0.373559949287, 0.261235304683, 0.405884763544, 0.384645076997, 0.275612693403, 0.367548912751, 0.301609240793], [0.49121356796, 0.349911768499, 0.0140163857025, 0.511247189891, 0.315396451391, -0.040035183695, 0.459491759478, 0.238122601938, -0.0760792258937], [-0.270118685166, 0.188059162761, 0.550179916507, -0.344981907231, -0.22451350832, 0.0962181922777, -0.269197212465, -0.485202768446, -0.320009482253], [0.430395084528, 0.503773319272, 0.354960354163, 0.409740953725, 0.384128914874, 0.204793330164, 0.235879393697, 0.13930928864, 0.0504629985334], [0.350299728348, 0.351926101177, 0.283669783477, 0.256620995659, 0.335292716766, 0.361338926455, 0.176519994606, 0.353179109917, 0.456326002211], [0.0617904569224, 0.167856065458, 0.268128587383, 0.247778414411, 0.379827333499, 0.35585351738, 0.445015401882, 0.50207193744, 0.337199838851], [-0.168942667627, -0.39668164284, -0.461571185071, -0.152619728233, -0.374477326755, -0.444583559457, -0.132427143411, -0.299845271436, -0.36390845586], [-0.0768078721995, 0.386944515971, 0.670114885382, -0.132716351275, 0.221477074178, 0.500206444874, -0.124954663448, 0.0306540538522, 0.248787021377], [0.336636985872, 0.245978553817, 0.0987586581331, 0.532401431335, 0.317340473784, -0.0139849003275, 0.589040121598, 0.272801562441, -0.103324842614], [0.361914833532, 0.177341890013, -0.0719299428144, 0.492381732534, 0.344534143409, 0.0252883812882, 0.517599075089, 0.433853806097, 0.120302179154], [0.680618681471, 0.341537593683, -0.120083988472, 0.412921878603, -0.0173031969166, -0.314577030322, 0.0888827304688, -0.220627986109, -0.281692534083], [-0.295763558053, -0.349747659974, -0.338224568612, -0.306867149687, -0.36314906397, -0.347813176283, -0.300738263118, -0.352506580255, -0.337759597075]]\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "import get_feature\n",
    "\n",
    "centroid = []\n",
    "count = 0\n",
    "temp = []\n",
    "n = 3*3\n",
    "num_data = 10000\n",
    "clusters = [16,32,64,128]\n",
    "#cluster = 16\n",
    "#centroid_file = r\"%s\\%dfeature_%d.txt\" %(pwd ,  cluster , num_data);\n",
    "for cluster in clusters:\n",
    "    centroid_file = r\"%s\\25_4_3_%d.txt\" %(pwd , cluster)\n",
    "    centroid.append(get_feature.read_features(centroid_file))\n",
    "'''\n",
    "data = np.loadtxt(centroid_file,delimiter=\"\\n\")\n",
    "for i in range(0 , len(data)):\n",
    "    temp.append(data[i])\n",
    "    count += 1\n",
    "    if(count == n):\n",
    "        centroid.append(temp)\n",
    "        temp = []\n",
    "        count = 0\n",
    "'''\n",
    "print(centroid[0])\n",
    "print(len(centroid[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the bag of features for each patch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from my_vlad import my_vlad\n",
    "\n",
    "# will load data as the patch size defined , 3 means 3*3 = 9 for each patch, and will return the dictionary included:\n",
    "# 'data'  (one patch)  , 'target' (the sample of this patch belongs to ) , 'filename' (the file comes from)\n",
    "bofs = []\n",
    "vlad = my_vlad(centroid[0])\n",
    "for file in train_X:\n",
    "    mydata,y = myData.load_sig_data(file , 3)\n",
    "    bofs.append(vlad.get_vlad(mydata['data']).flatten() )\n",
    "#print(bofs[0])\n",
    "#print(X_train_tf)\n",
    "#print(train_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used the bag of features of each image and the known lable to do classification with knn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=2, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import grid_search\n",
    "\n",
    "\n",
    "#knn_init = KNeighborsClassifier()\n",
    "#parameters = {'n_neighbors':[ 5, 10 , 15]}\n",
    "#knn = grid_search.GridSearchCV(knn_init, parameters)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 2)\n",
    "\n",
    "knn.fit(bofs, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bofs_test = []\n",
    "for file in test_X:\n",
    "    mydata,y = myData.load_sig_data(file , 3)\n",
    "    bofs_test.append(vlad.get_vlad(mydata['data']).flatten() )\n",
    "#print(bofs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14 13  8  3  4  3  8  3  8  3 14  7 13  4 13  3  6  8  1 13 14  1  6  8 13\n",
      "  5  6 20  5  3 13  6  8  6  3  8  1  8  4  3  3  3 22 22  3 20  6  5  8  9]\n",
      "[19 13 11  2 10 21 11 24  3  1 14  7 15  4 13  6 17 11 22 15 14  5  6  4 19\n",
      "  9  2  7  5 24 19  2 10  2 21 20 25 23 23 24 16  3 25  1 17 20 16 12 23  9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.22"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted = knn.predict(bofs_test)\n",
    "print(predicted)\n",
    "print(test_y)\n",
    "knn.score(bofs_test,test_y)\n",
    "\n",
    " \n",
    "#np.mean(predicted == test_y)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When used k = 16 and with 4 images of each sample, and with 25 samples, so totally have 100 images (knn neighbor with 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\howfungirl\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module has been deprecated in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.19.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.35999999999999999"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import run\n",
    "run.run(centroid , 0 , train_X , train_y ,test_X , test_y, method='knn' , n_nb = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "When used k = 32 and with 4 images of each sample, and with 25 samples, so totally have 100 images  (knn neighbor with 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\howfungirl\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module has been deprecated in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.19.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.20000000000000001"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import run\n",
    "run.run(centroid , 1 , train_X , train_y ,test_X , test_y, method='knn' , n_nb = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When used k = 16 and with 4 images of each sample, and with 25 samples, so totally have 100 images  (knn neighbor with 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.040000000000000001"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import run\n",
    "run.run(centroid , 0 , train_X , train_y ,test_X , test_y, method='knn' , n_nb = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When used k = 16 and with 4 images of each sample, and with 25 samples, so totally have 100 images  (knn neighbor with 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47999999999999998"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import run\n",
    "run.run(centroid , 0 , train_X , train_y ,test_X , test_y, method='knn' , n_nb = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used LinearSVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from Choose_target import Choose_Target\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "rng = np.random.RandomState(seed)\n",
    "permutation = rng.permutation(len(filelist))\n",
    "\n",
    "#print(permutation)\n",
    "#print(filelist[permutation])\n",
    "X, y = filelist[permutation], lable[permutation]\n",
    "select1 = Choose_Target(X, y)\n",
    "select1.select(1)\n",
    "X1 = select1.X\n",
    "y1 = select1.y\n",
    "\n",
    "# split our data into half of train data and half of text data randomly.\n",
    "train_X, test_X, train_y, test_y = train_test_split(X1, y1, train_size=0.75, random_state=seed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import run\n",
    "run.run(centroid , 0 , train_X , train_y ,test_X , test_y, method='LinearSVM' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
