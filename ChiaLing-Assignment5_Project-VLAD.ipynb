{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Data Partition as Training and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\patch_database\n",
      "..\\patch_database\\sample01_002_3x3\n",
      "..\\patch_database\\sample26_002_3x3\n",
      "[ 1  1  1  1  2  2  2  2  3  3  3  3  4  4  4  4  5  5  5  5  6  6  6  6  7\n",
      "  7  7  7  8  8  8  8  9  9  9  9 10 10 10 10 11 11 11 11 12 12 12 12 13 13\n",
      " 13 13 14 14 14 14 15 15 15 15 16 16 16 16 17 17 17 17 18 18 18 18 19 19 19\n",
      " 19 20 20 20 20 21 21 21 21 22 22 22 22 23 23 23 23 24 24 24 24 25 25 25 25\n",
      " 26 26 26 26 27 27 27 27 28 28 28 28 29 29 29 29 30 30 30 30 31 31 31 31 32\n",
      " 32 32 32 33 33 33 33 34 34 34 34 35 35 35 35 36 36 36 36 37 37 37 37 38 38\n",
      " 38 38 39 39 39 39 40 40 40 40 41 41 41 41 42 42 42 42 43 43 43 43 44 44 44\n",
      " 44 45 45 45 45 46 46 46 46 47 47 47 47 48 48 48 48 49 49 49 49 50 50 50 50\n",
      " 51 51 51 51 52 52 52 52 53 53 53 53 54 54 54 54 55 55 55 55 56 56 56 56 57\n",
      " 57 57 57 58 58 58 58 59 59 59 59 60 60 60 60 61 61 61 61]\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import image_norm_test as myData;\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "seed = 3071986   # set random seed\n",
    "rng = np.random.RandomState(seed)\n",
    "import re\n",
    "\n",
    "# fetch all the file in the directory path  defined#\n",
    "pwd = os.pardir;\n",
    "accessPath = r\"%s\\patch_database\" %pwd;\n",
    "filelist = []\n",
    "print(accessPath)\n",
    "\n",
    "def retrive_lable(filename):\n",
    "    new_name = filename.split('\\\\')[2].split('_')[0]\n",
    "    sample = int(re.findall(r'\\d+', new_name)[0])\n",
    "    return sample\n",
    "    \n",
    "def fetchFile(accessPath , numOfimageOfeachSample = 91):\n",
    "    filelist = []\n",
    "    lable = []\n",
    "    index = 0\n",
    "    i = 0\n",
    "    for file in os.listdir(accessPath):\n",
    "        if(index % 92 == i):\n",
    "            completeName = os.path.join(accessPath, file)  \n",
    "            filelist.append(completeName)\n",
    "            lable.append(retrive_lable(completeName))\n",
    "            if(i < numOfimageOfeachSample-1):  \n",
    "                i+=1\n",
    "            else:\n",
    "                i = 0\n",
    "        index +=1\n",
    "    return filelist , lable\n",
    "\n",
    "# in this way we can extract only certain number image of each samples, ex. 2 means only 2 images of one texture\n",
    "filelist , lable = fetchFile(accessPath , 4)\n",
    "print(filelist[0])\n",
    "print(filelist[100])\n",
    "filelist = np.array(filelist).transpose();\n",
    "lable = np.array(lable).transpose();\n",
    "print(lable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60 41 34  9 37 12 57 13 34 44  2 55  7 14 21 17 36  4 45 56  5 39 31 44 23\n",
      " 27  1  5 51 28 45 37 37 53 10  7 15 24 55 29 41 12  9 31 55 58 15 61 34 39\n",
      " 52 19 20 36 47 56 54 19 13  1 53 22  3  5 59 35 47 10  8  5 39 43 46 56  2\n",
      "  8 55 42 19 24 22 18 33 52  4 38 17 29 35 58 30 21 30 49 25  7 32 40 46 35\n",
      " 44 49  6 10 20 10 28 45 54 27 25 21 32 31 12 24 45 16  3 48 33 50]\n",
      "[42 46 56 50 58 33 23 42 26 24 30 16 31 43 17 40 20  3 57  2 59 26 47 35 51\n",
      " 41 11 18 40 49 22 54 29 15 16  8 50 27 14  6 28  6 26 13 22 16 48 25 25 32\n",
      " 36 32 58 15  2  9 48  3  4 60 38 33 47 59 61 38 59 60 36  6 14 43 52 40 11\n",
      " 19 23 49 39 30 53 23 57 37 42 51 54 43  7 21 46 51  1 17  1  4 28 48 34 29\n",
      " 61 13 38 18 52 57 50 44 61  8 41 14 53 27 26 11 20 60 18 12  9 11]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "rng = np.random.RandomState(seed)\n",
    "permutation = rng.permutation(len(filelist))\n",
    "\n",
    "#print(permutation)\n",
    "#print(filelist[permutation])\n",
    "\n",
    "X, y = filelist[permutation], lable[permutation]\n",
    "\n",
    "\n",
    "# split our data into half of train data and half of text data randomly.\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, train_size=0.5, random_state=seed)\n",
    "\n",
    "\n",
    "print(train_y)\n",
    "print(test_y)\n",
    "#print(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start to get the features learned from feed in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "def read_features(file_name):\n",
    "    features = list()\n",
    "\n",
    "    f = open(file_name)\n",
    "    for line in f.readlines():\n",
    "        feature_list = line.split(\" \")\n",
    "        feature = list()\n",
    "        for item in feature_list:\n",
    "            if item != '\\n':\n",
    "                feature.append(float(item))\n",
    "        features.append(feature)\n",
    "\n",
    "    f.close()\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrive centroids ( features ) from file stored in disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.359201245835, -0.350276323095, -0.284874560613, -0.387853317701, -0.356222505484, -0.265256232066, -0.382704706903, -0.336148081943, -0.244915257572], [0.358806866422, 0.290237064273, 0.279483142322, 0.341993511392, 0.295504326746, 0.315070116924, 0.35871147642, 0.356818418472, 0.386476904451], [-0.148784868061, 0.164467159988, 0.442070713355, -0.131500140521, 0.248641382045, 0.541956821763, -0.0695002573605, 0.294172120952, 0.539617973866], [-0.37811067742, -0.182680751809, 0.262972398401, -0.36602944445, 0.000716159993457, 0.476164862946, -0.207526841013, 0.192900624991, 0.55992543393], [0.211746974514, 0.364218791337, 0.373559949287, 0.261235304683, 0.405884763544, 0.384645076997, 0.275612693403, 0.367548912751, 0.301609240793], [0.49121356796, 0.349911768499, 0.0140163857025, 0.511247189891, 0.315396451391, -0.040035183695, 0.459491759478, 0.238122601938, -0.0760792258937], [-0.270118685166, 0.188059162761, 0.550179916507, -0.344981907231, -0.22451350832, 0.0962181922777, -0.269197212465, -0.485202768446, -0.320009482253], [0.430395084528, 0.503773319272, 0.354960354163, 0.409740953725, 0.384128914874, 0.204793330164, 0.235879393697, 0.13930928864, 0.0504629985334], [0.350299728348, 0.351926101177, 0.283669783477, 0.256620995659, 0.335292716766, 0.361338926455, 0.176519994606, 0.353179109917, 0.456326002211], [0.0617904569224, 0.167856065458, 0.268128587383, 0.247778414411, 0.379827333499, 0.35585351738, 0.445015401882, 0.50207193744, 0.337199838851], [-0.168942667627, -0.39668164284, -0.461571185071, -0.152619728233, -0.374477326755, -0.444583559457, -0.132427143411, -0.299845271436, -0.36390845586], [-0.0768078721995, 0.386944515971, 0.670114885382, -0.132716351275, 0.221477074178, 0.500206444874, -0.124954663448, 0.0306540538522, 0.248787021377], [0.336636985872, 0.245978553817, 0.0987586581331, 0.532401431335, 0.317340473784, -0.0139849003275, 0.589040121598, 0.272801562441, -0.103324842614], [0.361914833532, 0.177341890013, -0.0719299428144, 0.492381732534, 0.344534143409, 0.0252883812882, 0.517599075089, 0.433853806097, 0.120302179154], [0.680618681471, 0.341537593683, -0.120083988472, 0.412921878603, -0.0173031969166, -0.314577030322, 0.0888827304688, -0.220627986109, -0.281692534083], [-0.295763558053, -0.349747659974, -0.338224568612, -0.306867149687, -0.36314906397, -0.347813176283, -0.300738263118, -0.352506580255, -0.337759597075]]\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "centroid = []\n",
    "count = 0\n",
    "temp = []\n",
    "n = 3*3\n",
    "num_data = 10000\n",
    "clusters = [16,32,64,128]\n",
    "#cluster = 16\n",
    "#centroid_file = r\"%s\\%dfeature_%d.txt\" %(pwd ,  cluster , num_data);\n",
    "for cluster in clusters:\n",
    "    centroid_file = r\"%s\\25_4_3_%d.txt\" %(pwd , cluster)\n",
    "    centroid.append(read_features(centroid_file))\n",
    "'''\n",
    "data = np.loadtxt(centroid_file,delimiter=\"\\n\")\n",
    "for i in range(0 , len(data)):\n",
    "    temp.append(data[i])\n",
    "    count += 1\n",
    "    if(count == n):\n",
    "        centroid.append(temp)\n",
    "        temp = []\n",
    "        count = 0\n",
    "'''\n",
    "print(centroid[0])\n",
    "print(len(centroid[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the bag of features for each patch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n",
      "40000\n",
      "40000\n",
      "40000\n",
      "40000\n",
      "40000\n",
      "40000\n",
      "40000\n",
      "40000\n",
      "40000\n"
     ]
    }
   ],
   "source": [
    "from my_vlad import my_vlad\n",
    "\n",
    "# will load data as the patch size defined , 3 means 3*3 = 9 for each patch, and will return the dictionary included:\n",
    "# 'data'  (one patch)  , 'target' (the sample of this patch belongs to ) , 'filename' (the file comes from)\n",
    "bofs = []\n",
    "vlad = my_vlad(centroid[0])\n",
    "for file in train_X[0:10]:\n",
    "    mydata,y = myData.load_sig_data(file , 3)\n",
    "    bofs.append(vlad.get_vlad(mydata['data']).flatten() )\n",
    "#print(bofs[0])\n",
    "#print(X_train_tf)\n",
    "#print(train_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used the bag of features of each image and the known lable to do classification with knn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\howfungirl\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module has been deprecated in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.19.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=2, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import grid_search\n",
    "\n",
    "\n",
    "#knn_init = KNeighborsClassifier()\n",
    "#parameters = {'n_neighbors':[ 5, 10 , 15]}\n",
    "#knn = grid_search.GridSearchCV(knn_init, parameters)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 2)\n",
    "\n",
    "knn.fit(bofs, train_y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n",
      "40000\n",
      "40000\n",
      "40000\n",
      "40000\n",
      "40000\n",
      "40000\n",
      "40000\n",
      "40000\n",
      "40000\n"
     ]
    }
   ],
   "source": [
    "bofs_test = []\n",
    "for data in test_X[0:10]:\n",
    "    mydata,y = myData.load_sig_data(file , 3)\n",
    "    bofs_test.append(vlad.get_vlad(mydata['data']).flatten() )\n",
    "#print(bofs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34 34 34 34 34 34 34 34 34 34]\n",
      "[42 46 56 50 58 33 23 42 26 24]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted = knn.predict(bofs_test)\n",
    "print(predicted)\n",
    "print(test_y[0:10])\n",
    "knn.score(bofs_test,test_y[0:10])\n",
    "\n",
    " \n",
    "#np.mean(predicted == test_y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
