{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Data Partition as Training and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\patch_database\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import image_norm_test as myData;\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "seed = 3071986   # set random seed\n",
    "rng = np.random.RandomState(seed)\n",
    "\n",
    "# fetch all the file in the directory path  defined#\n",
    "pwd = os.pardir;\n",
    "accessPath = r\"%s\\patch_database\" %pwd;\n",
    "filelist = []\n",
    "print(accessPath)\n",
    "def fetchFile(accessPath , numOfimageOfeachSample = 91):\n",
    "    filelist = []\n",
    "    index = 0\n",
    "    i = 0\n",
    "    for file in os.listdir(accessPath):\n",
    "        if(index % 92 == i):\n",
    "            completeName = os.path.join(accessPath, file)  \n",
    "            filelist.append(completeName)\n",
    "            if(i < numOfimageOfeachSample-1):  \n",
    "                i+=1\n",
    "            else:\n",
    "                i = 0\n",
    "        index +=1\n",
    "    return filelist\n",
    "\n",
    "# in this way we can extract only certain number image of each samples, ex. 2 means only 2 images of one texture\n",
    "filelist = fetchFile(accessPath , 4)\n",
    "#print(filelist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# will load data as the patch size defined , 3 means 3*3 = 9 for each patch, and will return the dictionary included:\n",
    "# 'data'  (one patch)  , 'target' (the sample of this patch belongs to ) , 'filename' (the file comes from)\n",
    "mydata = myData.load_data(filelist[0:25] , 3)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "#print(len(mydata))\n",
    "#print(mydata.keys())\n",
    "\n",
    "\n",
    "X, y = mydata['data'], mydata['target']\n",
    "#print(len(X))\n",
    "#print(X[0])\n",
    "#print(X[0])\n",
    "\n",
    "# Normalized all the data that we are going to use\n",
    "x_norm = scaler.fit_transform(X)\n",
    "#print(mydata['filename'])\n",
    "#print(y)\n",
    "\n",
    "# split our data into half of train data and half of text data randomly.\n",
    "train_X, test_X, train_y, test_y = train_test_split(x_norm, y, train_size=0.5, random_state=seed)\n",
    "\n",
    "\n",
    "#print(train_y)\n",
    "#print(test_y)\n",
    "#print(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start to get the features learned from feed in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import learnvocabulary as LV\n",
    "\n",
    "'''\n",
    "start_time = time.time()\n",
    "#print(train_X[0])\n",
    "#print(len(train_X)*0.5)  #100000.0\n",
    "num_data = 10000\n",
    "clusters = [2,16,32,64,128,256]\n",
    "for cluster in clusters:\n",
    "    start_time = time.time()\n",
    "    centroid = LV.learnvocabulary(train_X, num_data ,rng , cluster_num = cluster , T = 50)\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    centroid_file = r\"%s\\%dfeature_%d.txt\" %(pwd ,  cluster , num_data);\n",
    "    f = open(centroid_file, 'w')\n",
    "    for items in centroid:\n",
    "        for item in items:\n",
    "            f.write(\"%s\\n\" % item  )\n",
    "    f.close()\n",
    "#print(centroid)\n",
    "'''\n",
    "def read_features(file_name):\n",
    "    features = list()\n",
    "\n",
    "    f = open(file_name)\n",
    "    for line in f.readlines():\n",
    "        feature_list = line.split(\" \")\n",
    "        feature = list()\n",
    "        for item in feature_list:\n",
    "            if item != '\\n':\n",
    "                feature.append(float(item))\n",
    "        features.append(feature)\n",
    "\n",
    "    f.close()\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrive centroids ( features ) from file stored in disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata = np.loadtxt(centroid_file,delimiter=\"\\n\")\\nfor i in range(0 , len(data)):\\n    temp.append(data[i])\\n    count += 1\\n    if(count == n):\\n        centroid.append(temp)\\n        temp = []\\n        count = 0\\n'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroid = []\n",
    "count = 0\n",
    "temp = []\n",
    "n = 3*3\n",
    "num_data = 10000\n",
    "clusters = [16,32,64,128]\n",
    "#cluster = 16\n",
    "#centroid_file = r\"%s\\%dfeature_%d.txt\" %(pwd ,  cluster , num_data);\n",
    "for cluster in clusters:\n",
    "    centroid_file = r\"%s\\25_4_3_%d.txt\" %(pwd , cluster)\n",
    "    centroid.append(read_features(centroid_file))\n",
    "'''\n",
    "data = np.loadtxt(centroid_file,delimiter=\"\\n\")\n",
    "for i in range(0 , len(data)):\n",
    "    temp.append(data[i])\n",
    "    count += 1\n",
    "    if(count == n):\n",
    "        centroid.append(temp)\n",
    "        temp = []\n",
    "        count = 0\n",
    "'''\n",
    "#print(centroid)\n",
    "#print(len(centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the bag of features for each patch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.715065569942035, -4.7515505539290066, -2.8952694109471411, -0.50492700395561485, -4.6420701326577714, -3.6608258125979916, 1.9651125215511773, -4.2568858349784309, -4.6463117799052833, -4.4033474297375799, 4.3624538635138723, -2.5407944422441391, -3.6599234548550377, -3.9295758218665875, -0.95387961022450596, 4.7359841337155419]\n"
     ]
    }
   ],
   "source": [
    "import bagOfFeature\n",
    "bofs = []\n",
    "dot = bagOfFeature.getbofs(centroid[0])\n",
    "for data in train_X[0:10000]:\n",
    "    bofs.append(dot.getbof(data) )\n",
    "print(bofs[0])\n",
    "#print(X_train_tf)\n",
    "#print(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "divide the number of occurrences of each word in a document by the total number of words in the document\n",
    "(Term Frequency times Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(bofs)\n",
    "X_train_tf = tf_transformer.transform(bofs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used the bag of features of each patch and the known lable to do classification with knn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=64, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import grid_search\n",
    "\n",
    "#knn_init = KNeighborsClassifier()\n",
    "#parameters = {'n_neighbors':[ 5, 10 , 15]}\n",
    "#knn = grid_search.GridSearchCV(knn_init, parameters)\n",
    "knn = KNeighborsClassifier(n_neighbors = 64)\n",
    "\n",
    "knn.fit(X_train_tf, train_y[0:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bofs_test = []\n",
    "for data in test_X[0:5000]:\n",
    "    bofs_test.append(dot.getbof(data) )\n",
    "#print(bofs)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(bofs_test)\n",
    "X_test_tf = tf_transformer.transform(bofs_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25519999999999998"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#predicted = knn.predict(X_test_tf)\n",
    "knn.score(X_test_tf,test_y[0:5000])\n",
    "\n",
    " \n",
    "#np.mean(predicted == test_y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
